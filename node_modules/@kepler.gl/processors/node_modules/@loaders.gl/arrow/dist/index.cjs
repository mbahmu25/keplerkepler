"use strict";
var __create = Object.create;
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __getProtoOf = Object.getPrototypeOf;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toESM = (mod, isNodeMode, target) => (target = mod != null ? __create(__getProtoOf(mod)) : {}, __copyProps(
  // If the importer is in node compatibility mode or this is not an ESM
  // file that has been converted to a CommonJS file using a Babel-
  // compatible transform (i.e. "__esModule" has not been set), then set
  // "default" to the CommonJS "module.exports" for node compatibility.
  isNodeMode || !mod || !mod.__esModule ? __defProp(target, "default", { value: mod, enumerable: true }) : target,
  mod
));
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// dist/index.js
var dist_exports = {};
__export(dist_exports, {
  ArrowLoader: () => ArrowLoader,
  ArrowWorkerLoader: () => ArrowWorkerLoader,
  ArrowWriter: () => ArrowWriter,
  GeoArrowLoader: () => GeoArrowLoader,
  GeoArrowWorkerLoader: () => GeoArrowWorkerLoader,
  TriangulationWorker: () => TriangulationWorker,
  VECTOR_TYPES: () => VECTOR_TYPES,
  convertArrowToSchema: () => convertArrowToSchema,
  convertArrowToTable: () => convertArrowToTable,
  convertSchemaToArrow: () => convertSchemaToArrow,
  convertTableToArrow: () => convertTableToArrow,
  deserializeArrowField: () => deserializeArrowField,
  deserializeArrowMetadata: () => deserializeArrowMetadata,
  deserializeArrowSchema: () => deserializeArrowSchema,
  deserializeArrowType: () => deserializeArrowType,
  getBinaryGeometriesFromArrow: () => getBinaryGeometriesFromArrow,
  getBinaryGeometryTemplate: () => getBinaryGeometryTemplate,
  getMeanCentersFromBinaryGeometries: () => getMeanCentersFromBinaryGeometries,
  getTriangleIndices: () => getTriangleIndices,
  hardClone: () => hardClone,
  parseGeoArrowOnWorker: () => parseGeoArrowOnWorker,
  parseGeometryFromArrow: () => parseGeometryFromArrow,
  serializeArrowField: () => serializeArrowField,
  serializeArrowMetadata: () => serializeArrowMetadata,
  serializeArrowSchema: () => serializeArrowSchema,
  serializeArrowType: () => serializeArrowType,
  triangulateOnWorker: () => triangulateOnWorker,
  updateBoundsFromGeoArrowSamples: () => updateBoundsFromGeoArrowSamples
});
module.exports = __toCommonJS(dist_exports);

// dist/lib/types.js
var VECTOR_TYPES;
(function(VECTOR_TYPES2) {
  VECTOR_TYPES2[VECTOR_TYPES2["FLOAT"] = 0] = "FLOAT";
  VECTOR_TYPES2[VECTOR_TYPES2["DATE"] = 1] = "DATE";
})(VECTOR_TYPES || (VECTOR_TYPES = {}));

// dist/schema/arrow-table-batch.js
var import_schema = require("@loaders.gl/schema");
var arrow = __toESM(require("apache-arrow"), 1);
var ArrowTableBatchAggregator = class extends import_schema.ColumnarTableBatchAggregator {
  arrowSchema;
  constructor(schema, options) {
    super(schema, options);
    this.arrowSchema = null;
  }
  getBatch() {
    const batch = super.getBatch();
    if (batch) {
      this.arrowSchema = this.arrowSchema || getArrowSchema(batch.schema);
      const arrowVectors = getArrowVectors(this.arrowSchema, batch.data);
      const recordBatch = new arrow.RecordBatch(this.arrowSchema, arrow.makeData({
        type: new arrow.Struct(this.arrowSchema.fields),
        children: arrowVectors.map(({ data }) => data[0])
      }));
      return {
        shape: "arrow-table",
        batchType: "data",
        data: new arrow.Table([recordBatch]),
        length: batch.length
      };
    }
    return null;
  }
};
function getArrowSchema(schema) {
  const arrowFields = [];
  for (const key in schema) {
    const field = schema[key];
    if (field.type === Float32Array) {
      const metadata = /* @__PURE__ */ new Map();
      const arrowField = new arrow.Field(field.name, new arrow.Float32(), field.nullable, metadata);
      arrowFields.push(arrowField);
    }
  }
  if (arrowFields.length === 0) {
    throw new Error("No arrow convertible fields");
  }
  return new arrow.Schema(arrowFields);
}
function getArrowVectors(arrowSchema, data) {
  const arrowVectors = [];
  for (const field of arrowSchema.fields) {
    const vector = data[field.name];
    if (vector instanceof Float32Array) {
      const arrowVector = arrow.makeVector(vector);
      arrowVectors.push(arrowVector);
    }
  }
  if (arrowSchema.fields.length !== arrowVectors.length) {
    throw new Error("Some columns not arrow convertible");
  }
  return arrowVectors;
}

// dist/index.js
var import_schema4 = require("@loaders.gl/schema");

// dist/exports/arrow-loader.js
var VERSION = true ? "4.3.3" : "latest";
var ArrowWorkerLoader = {
  dataType: null,
  batchType: null,
  name: "Apache Arrow",
  id: "arrow",
  module: "arrow",
  version: VERSION,
  // worker: true,
  category: "table",
  extensions: ["arrow", "feather"],
  mimeTypes: [
    "application/vnd.apache.arrow.file",
    "application/vnd.apache.arrow.stream",
    "application/octet-stream"
  ],
  binary: true,
  tests: ["ARROW"],
  options: {
    arrow: {
      shape: "columnar-table"
    }
  }
};

// dist/lib/parsers/parse-arrow.js
var arrow3 = __toESM(require("apache-arrow"), 1);

// dist/lib/tables/convert-arrow-to-table.js
var import_schema2 = require("@loaders.gl/schema");
var import_gis2 = require("@loaders.gl/gis");

// dist/lib/tables/convert-arrow-schema.js
var arrow2 = __toESM(require("apache-arrow"), 1);
function convertArrowToSchema(arrowSchema) {
  return serializeArrowSchema(arrowSchema);
}
function convertSchemaToArrow(schema) {
  return deserializeArrowSchema(schema);
}
function serializeArrowSchema(arrowSchema) {
  return {
    fields: arrowSchema.fields.map((arrowField) => serializeArrowField(arrowField)),
    metadata: serializeArrowMetadata(arrowSchema.metadata)
  };
}
function deserializeArrowSchema(schema) {
  return new arrow2.Schema(schema.fields.map((field) => deserializeArrowField(field)), deserializeArrowMetadata(schema.metadata));
}
function serializeArrowMetadata(arrowMetadata) {
  return Object.fromEntries(arrowMetadata);
}
function deserializeArrowMetadata(metadata) {
  return metadata ? new Map(Object.entries(metadata)) : /* @__PURE__ */ new Map();
}
function serializeArrowField(field) {
  return {
    name: field.name,
    type: serializeArrowType(field.type),
    nullable: field.nullable,
    metadata: serializeArrowMetadata(field.metadata)
  };
}
function deserializeArrowField(field) {
  return new arrow2.Field(field.name, deserializeArrowType(field.type), field.nullable, deserializeArrowMetadata(field.metadata));
}
function serializeArrowType(arrowType) {
  switch (arrowType.constructor) {
    case arrow2.Null:
      return "null";
    case arrow2.Binary:
      return "binary";
    case arrow2.Bool:
      return "bool";
    case arrow2.Int:
      const intType = arrowType;
      return `${intType.isSigned ? "u" : ""}int${intType.bitWidth}`;
    case arrow2.Int8:
      return "int8";
    case arrow2.Int16:
      return "int16";
    case arrow2.Int32:
      return "int32";
    case arrow2.Int64:
      return "int64";
    case arrow2.Uint8:
      return "uint8";
    case arrow2.Uint16:
      return "uint16";
    case arrow2.Uint32:
      return "uint32";
    case arrow2.Uint64:
      return "uint64";
    case arrow2.Float:
      const precision = arrowType.precision;
      switch (precision) {
        case arrow2.Precision.HALF:
          return "float16";
        case arrow2.Precision.SINGLE:
          return "float32";
        case arrow2.Precision.DOUBLE:
          return "float64";
        default:
          return "float16";
      }
    case arrow2.Float16:
      return "float16";
    case arrow2.Float32:
      return "float32";
    case arrow2.Float64:
      return "float64";
    case arrow2.Utf8:
      return "utf8";
    case arrow2.Decimal:
      const decimal = arrowType;
      return {
        type: "decimal",
        bitWidth: decimal.bitWidth,
        precision: decimal.precision,
        scale: decimal.scale
      };
    case arrow2.Date_:
      const dateUnit = arrowType.unit;
      return dateUnit === arrow2.DateUnit.DAY ? "date-day" : "date-millisecond";
    case arrow2.DateDay:
      return "date-day";
    case arrow2.DateMillisecond:
      return "date-millisecond";
    case arrow2.Time:
      const timeUnit = arrowType.unit;
      switch (timeUnit) {
        case arrow2.TimeUnit.SECOND:
          return "time-second";
        case arrow2.TimeUnit.MILLISECOND:
          return "time-millisecond";
        case arrow2.TimeUnit.MICROSECOND:
          return "time-microsecond";
        case arrow2.TimeUnit.NANOSECOND:
          return "time-nanosecond";
        default:
          return "time-second";
      }
    case arrow2.TimeMillisecond:
      return "time-millisecond";
    case arrow2.TimeSecond:
      return "time-second";
    case arrow2.TimeMicrosecond:
      return "time-microsecond";
    case arrow2.TimeNanosecond:
      return "time-nanosecond";
    case arrow2.Timestamp:
      const timeStampUnit = arrowType.unit;
      switch (timeStampUnit) {
        case arrow2.TimeUnit.SECOND:
          return "timestamp-second";
        case arrow2.TimeUnit.MILLISECOND:
          return "timestamp-millisecond";
        case arrow2.TimeUnit.MICROSECOND:
          return "timestamp-microsecond";
        case arrow2.TimeUnit.NANOSECOND:
          return "timestamp-nanosecond";
        default:
          return "timestamp-second";
      }
    case arrow2.TimestampSecond:
      return "timestamp-second";
    case arrow2.TimestampMillisecond:
      return "timestamp-millisecond";
    case arrow2.TimestampMicrosecond:
      return "timestamp-microsecond";
    case arrow2.TimestampNanosecond:
      return "timestamp-nanosecond";
    case arrow2.Interval:
      const intervalUnit = arrowType.unit;
      switch (intervalUnit) {
        case arrow2.IntervalUnit.DAY_TIME:
          return "interval-daytime";
        case arrow2.IntervalUnit.YEAR_MONTH:
          return "interval-yearmonth";
        default:
          return "interval-daytime";
      }
    case arrow2.IntervalDayTime:
      return "interval-daytime";
    case arrow2.IntervalYearMonth:
      return "interval-yearmonth";
    case arrow2.Map_:
      const mapType = arrowType;
      return {
        type: "map",
        keysSorted: mapType.keysSorted,
        children: mapType.children.map((arrowField) => serializeArrowField(arrowField))
      };
    case arrow2.List:
      const listType = arrowType;
      const listField = listType.valueField;
      return {
        type: "list",
        children: [serializeArrowField(listField)]
      };
    case arrow2.FixedSizeList:
      const fixedSizeList = arrowType;
      return {
        type: "fixed-size-list",
        listSize: fixedSizeList.listSize,
        children: [serializeArrowField(fixedSizeList.children[0])]
      };
    case arrow2.Struct:
      const structType = arrowType;
      return {
        type: "struct",
        children: structType.children.map((arrowField) => serializeArrowField(arrowField))
      };
    default:
      throw new Error(`arrow type not supported: ${arrowType.constructor.name}`);
  }
}
function deserializeArrowType(dataType) {
  if (typeof dataType === "object") {
    switch (dataType.type) {
      case "decimal":
        return new arrow2.Decimal(dataType.precision, dataType.scale, dataType.bitWidth);
      case "map":
        let children = dataType.children.map((arrowField) => deserializeArrowField(arrowField));
        return new arrow2.Map_(children, dataType.keysSorted);
      case "list":
        const field = deserializeArrowField(dataType.children[0]);
        return new arrow2.List(field);
      case "fixed-size-list":
        const child = deserializeArrowField(dataType.children[0]);
        return new arrow2.FixedSizeList(dataType.listSize, child);
      case "struct":
        children = dataType.children.map((arrowField) => deserializeArrowField(arrowField));
        return new arrow2.Struct(children);
      default:
        throw new Error("array type not supported");
    }
  }
  switch (dataType) {
    case "null":
      return new arrow2.Null();
    case "binary":
      return new arrow2.Binary();
    case "bool":
      return new arrow2.Bool();
    case "int8":
      return new arrow2.Int8();
    case "int16":
      return new arrow2.Int16();
    case "int32":
      return new arrow2.Int32();
    case "int64":
      return new arrow2.Int64();
    case "uint8":
      return new arrow2.Uint8();
    case "uint16":
      return new arrow2.Uint16();
    case "uint32":
      return new arrow2.Uint32();
    case "uint64":
      return new arrow2.Uint64();
    case "float16":
      return new arrow2.Float16();
    case "float32":
      return new arrow2.Float32();
    case "float64":
      return new arrow2.Float64();
    case "utf8":
      return new arrow2.Utf8();
    case "date-day":
      return new arrow2.DateDay();
    case "date-millisecond":
      return new arrow2.DateMillisecond();
    case "time-second":
      return new arrow2.TimeSecond();
    case "time-millisecond":
      return new arrow2.TimeMillisecond();
    case "time-microsecond":
      return new arrow2.TimeMicrosecond();
    case "time-nanosecond":
      return new arrow2.TimeNanosecond();
    case "timestamp-second":
      return new arrow2.TimestampSecond();
    case "timestamp-millisecond":
      return new arrow2.TimestampMillisecond();
    case "timestamp-microsecond":
      return new arrow2.TimestampMicrosecond();
    case "timestamp-nanosecond":
      return new arrow2.TimestampNanosecond();
    case "interval-daytime":
      return new arrow2.IntervalDayTime();
    case "interval-yearmonth":
      return new arrow2.IntervalYearMonth();
    default:
      throw new Error("array type not supported");
  }
}

// dist/lib/geoarrow/convert-geoarrow-to-geojson-geometry.js
var import_gis = require("@loaders.gl/gis");
var import_wkt = require("@loaders.gl/wkt");
function parseGeometryFromArrow(arrowCellValue, encoding) {
  encoding = encoding == null ? void 0 : encoding.toLowerCase();
  if (!encoding || !arrowCellValue) {
    return null;
  }
  let geometry;
  switch (encoding) {
    case "geoarrow.multipolygon":
      geometry = arrowMultiPolygonToFeature(arrowCellValue);
      break;
    case "geoarrow.polygon":
      geometry = arrowPolygonToFeature(arrowCellValue);
      break;
    case "geoarrow.multipoint":
      geometry = arrowMultiPointToFeature(arrowCellValue);
      break;
    case "geoarrow.point":
      geometry = arrowPointToFeature(arrowCellValue);
      break;
    case "geoarrow.multilinestring":
      geometry = arrowMultiLineStringToFeature(arrowCellValue);
      break;
    case "geoarrow.linestring":
      geometry = arrowLineStringToFeature(arrowCellValue);
      break;
    case "geoarrow.wkb":
      geometry = arrowWKBToFeature(arrowCellValue);
      break;
    case "geoarrow.wkt":
      geometry = arrowWKTToFeature(arrowCellValue);
      break;
    default: {
      throw Error(`GeoArrow encoding not supported ${encoding}`);
    }
  }
  return geometry;
}
function arrowWKBToFeature(arrowCellValue) {
  var _a, _b;
  const arrayBuffer = arrowCellValue.buffer.slice(arrowCellValue.byteOffset, arrowCellValue.byteOffset + arrowCellValue.byteLength);
  const binaryGeometry = (_b = (_a = import_wkt.WKBLoader).parseSync) == null ? void 0 : _b.call(_a, arrayBuffer);
  const geometry = (0, import_gis.binaryToGeometry)(binaryGeometry);
  return geometry;
}
function arrowWKTToFeature(arrowCellValue) {
  var _a, _b;
  const string = arrowCellValue;
  return (_b = (_a = import_wkt.WKTLoader).parseTextSync) == null ? void 0 : _b.call(_a, string);
}
function arrowMultiPolygonToFeature(arrowMultiPolygon) {
  const multiPolygon = [];
  for (let m = 0; m < arrowMultiPolygon.length; m++) {
    const arrowPolygon = arrowMultiPolygon.get(m);
    const polygon = [];
    for (let i = 0; arrowPolygon && i < (arrowPolygon == null ? void 0 : arrowPolygon.length); i++) {
      const arrowRing = arrowPolygon == null ? void 0 : arrowPolygon.get(i);
      const ring = [];
      for (let j = 0; arrowRing && j < arrowRing.length; j++) {
        const arrowCoord = arrowRing.get(j);
        const coord = Array.from(arrowCoord);
        ring.push(coord);
      }
      polygon.push(ring);
    }
    multiPolygon.push(polygon);
  }
  const geometry = {
    type: "MultiPolygon",
    coordinates: multiPolygon
  };
  return geometry;
}
function arrowPolygonToFeature(arrowPolygon) {
  const polygon = [];
  for (let i = 0; arrowPolygon && i < arrowPolygon.length; i++) {
    const arrowRing = arrowPolygon.get(i);
    const ring = [];
    for (let j = 0; arrowRing && j < arrowRing.length; j++) {
      const arrowCoord = arrowRing.get(j);
      const coords = Array.from(arrowCoord);
      ring.push(coords);
    }
    polygon.push(ring);
  }
  const geometry = {
    type: "Polygon",
    coordinates: polygon
  };
  return geometry;
}
function arrowMultiPointToFeature(arrowMultiPoint) {
  const multiPoint = [];
  for (let i = 0; arrowMultiPoint && i < arrowMultiPoint.length; i++) {
    const arrowPoint = arrowMultiPoint.get(i);
    if (arrowPoint) {
      const coord = Array.from(arrowPoint);
      multiPoint.push(coord);
    }
  }
  return {
    type: "MultiPoint",
    coordinates: multiPoint
  };
}
function arrowPointToFeature(arrowPoint) {
  const point = Array.from(arrowPoint);
  return {
    type: "Point",
    coordinates: point
  };
}
function arrowMultiLineStringToFeature(arrowMultiLineString) {
  const multiLineString = [];
  for (let i = 0; arrowMultiLineString && i < arrowMultiLineString.length; i++) {
    const arrowLineString = arrowMultiLineString.get(i);
    const lineString = [];
    for (let j = 0; arrowLineString && j < arrowLineString.length; j++) {
      const arrowCoord = arrowLineString.get(j);
      if (arrowCoord) {
        const coords = Array.from(arrowCoord);
        lineString.push(coords);
      }
    }
    multiLineString.push(lineString);
  }
  return {
    type: "MultiLineString",
    coordinates: multiLineString
  };
}
function arrowLineStringToFeature(arrowLineString) {
  const lineString = [];
  for (let i = 0; arrowLineString && i < arrowLineString.length; i++) {
    const arrowCoord = arrowLineString.get(i);
    if (arrowCoord) {
      const coords = Array.from(arrowCoord);
      lineString.push(coords);
    }
  }
  return {
    type: "LineString",
    coordinates: lineString
  };
}

// dist/lib/tables/convert-arrow-to-table.js
function convertArrowToTable(arrowTable, shape) {
  switch (shape) {
    case "arrow-table":
      return convertArrowToArrowTable(arrowTable);
    case "array-row-table":
      return convertArrowToArrayRowTable(arrowTable);
    case "object-row-table":
      return convertArrowToObjectRowTable(arrowTable);
    case "columnar-table":
      return convertArrowToColumnarTable(arrowTable);
    case "geojson-table":
      return convertArrowToGeoJSONTable(arrowTable);
    default:
      throw new Error(shape);
  }
}
function convertArrowToArrowTable(arrowTable) {
  return {
    shape: "arrow-table",
    schema: convertArrowToSchema(arrowTable.schema),
    data: arrowTable
  };
}
function convertArrowToArrayRowTable(arrowTable) {
  const columnarTable = convertArrowToColumnarTable(arrowTable);
  return (0, import_schema2.convertTable)(columnarTable, "array-row-table");
}
function convertArrowToObjectRowTable(arrowTable) {
  const columnarTable = convertArrowToColumnarTable(arrowTable);
  return (0, import_schema2.convertTable)(columnarTable, "object-row-table");
}
function convertArrowToColumnarTable(arrowTable) {
  const columns = {};
  for (const field of arrowTable.schema.fields) {
    const arrowColumn = arrowTable.getChild(field.name);
    const values = arrowColumn == null ? void 0 : arrowColumn.toArray();
    columns[field.name] = values;
  }
  const schema = convertArrowToSchema(arrowTable.schema);
  return {
    shape: "columnar-table",
    schema,
    data: columns
  };
}
function convertArrowToGeoJSONTable(arrowTable) {
  var _a;
  const schema = convertArrowToSchema(arrowTable.schema);
  const geometryColumns = (0, import_gis2.getGeometryColumnsFromSchema)(schema);
  const encoding = geometryColumns.geometry.encoding;
  const features = [];
  const propertyColumnNames = arrowTable.schema.fields.map((field) => field.name).filter((name) => !(name in geometryColumns));
  const propertiesTable = arrowTable.select(propertyColumnNames);
  const arrowGeometryColumn = arrowTable.getChild("geometry");
  for (let row = 0; row < arrowTable.numRows; row++) {
    const arrowGeometry = arrowGeometryColumn == null ? void 0 : arrowGeometryColumn.get(row);
    const feature = parseGeometryFromArrow(arrowGeometry, encoding);
    if (feature) {
      const properties = ((_a = propertiesTable.get(row)) == null ? void 0 : _a.toJSON()) || {};
      features.push({ type: "Feature", geometry: feature, properties });
    }
  }
  return {
    shape: "geojson-table",
    type: "FeatureCollection",
    schema,
    features
  };
}

// dist/lib/parsers/parse-arrow.js
function parseArrowSync(arrayBuffer, options) {
  const shape = (options == null ? void 0 : options.shape) || "arrow-table";
  const arrowTable = arrow3.tableFromIPC([new Uint8Array(arrayBuffer)]);
  return convertArrowToTable(arrowTable, shape);
}
function parseArrowInBatches(asyncIterator, options) {
  async function* makeArrowAsyncIterator() {
    var _a, _b;
    const readers = arrow3.RecordBatchReader.readAll(asyncIterator);
    for await (const reader of readers) {
      for await (const recordBatch of reader) {
        if (((_a = options == null ? void 0 : options.arrow) == null ? void 0 : _a.batchDebounceMs) !== void 0 && ((_b = options == null ? void 0 : options.arrow) == null ? void 0 : _b.batchDebounceMs) > 0) {
          await new Promise((resolve) => {
            var _a2;
            return setTimeout(resolve, ((_a2 = options.arrow) == null ? void 0 : _a2.batchDebounceMs) || 0);
          });
        }
        const arrowTabledBatch = {
          shape: "arrow-table",
          batchType: "data",
          data: new arrow3.Table([recordBatch]),
          length: recordBatch.data.length
        };
        yield arrowTabledBatch;
      }
      break;
    }
  }
  return makeArrowAsyncIterator();
}

// dist/arrow-loader.js
var ArrowLoader = {
  ...ArrowWorkerLoader,
  parse: async (arraybuffer, options) => parseArrowSync(arraybuffer, options == null ? void 0 : options.arrow),
  parseSync: (arraybuffer, options) => parseArrowSync(arraybuffer, options == null ? void 0 : options.arrow),
  parseInBatches: parseArrowInBatches
};

// dist/lib/encoders/encode-arrow.js
var arrow4 = __toESM(require("apache-arrow"), 1);
function encodeArrowSync(data) {
  const vectors = {};
  for (const arrayData of data) {
    const arrayVector = createVector(arrayData.array, arrayData.type);
    vectors[arrayData.name] = arrayVector;
  }
  const table = new arrow4.Table(vectors);
  const arrowBuffer = arrow4.tableToIPC(table);
  return arrowBuffer;
}
function createVector(array, type) {
  switch (type) {
    case VECTOR_TYPES.DATE:
      return arrow4.vectorFromArray(array);
    case VECTOR_TYPES.FLOAT:
    default:
      return arrow4.vectorFromArray(array);
  }
}

// dist/arrow-writer.js
var VERSION2 = true ? "4.3.3" : "latest";
var ArrowWriter = {
  name: "Apache Arrow",
  id: "arrow",
  module: "arrow",
  version: VERSION2,
  extensions: ["arrow", "feather"],
  mimeTypes: [
    "application/vnd.apache.arrow.file",
    "application/vnd.apache.arrow.stream",
    "application/octet-stream"
  ],
  binary: true,
  options: {},
  encode: async function encodeArrow(data, options) {
    return encodeArrowSync(data);
  },
  encodeSync(data, options) {
    return encodeArrowSync(data);
  }
};

// dist/exports/geoarrow-loader.js
var GeoArrowWorkerLoader = {
  ...ArrowWorkerLoader,
  options: {
    arrow: {
      shape: "arrow-table"
    }
  }
};

// dist/lib/parsers/parse-geoarrow.js
function parseGeoArrowSync(arrayBuffer, options) {
  const table = parseArrowSync(arrayBuffer, { shape: "arrow-table" });
  switch (options == null ? void 0 : options.shape) {
    case "geojson-table":
      return convertArrowToTable(table.data, "geojson-table");
    default:
      return table;
  }
}
function parseGeoArrowInBatches(asyncIterator) {
  return parseArrowInBatches(asyncIterator);
}

// dist/geoarrow-loader.js
var GeoArrowLoader = {
  ...GeoArrowWorkerLoader,
  parse: async (arraybuffer, options) => parseGeoArrowSync(arraybuffer, options == null ? void 0 : options.arrow),
  parseSync: (arraybuffer, options) => parseGeoArrowSync(arraybuffer, options == null ? void 0 : options.arrow),
  parseInBatches: parseGeoArrowInBatches
};

// dist/lib/tables/convert-table-to-arrow.js
var arrow5 = __toESM(require("apache-arrow"), 1);
var import_schema3 = require("@loaders.gl/schema");
function convertTableToArrow(table, options) {
  switch (table.shape) {
    case "arrow-table":
      return table.data;
    case "columnar-table":
    default:
      const arrowBatchIterator = makeTableToArrowBatchesIterator(table, options);
      return new arrow5.Table(arrowBatchIterator);
  }
}
function* makeTableToArrowBatchesIterator(table, options) {
  const arrowSchema = deserializeArrowSchema(table.schema);
  const length = (0, import_schema3.getTableLength)(table);
  const numColumns = (0, import_schema3.getTableNumCols)(table);
  const batchSize = (options == null ? void 0 : options.batchSize) || length;
  const builders = arrowSchema == null ? void 0 : arrowSchema.fields.map((arrowField) => arrow5.makeBuilder(arrowField));
  const structField = new arrow5.Struct(arrowSchema.fields);
  let batchLength = 0;
  for (let rowIndex = 0; rowIndex < length; rowIndex++) {
    for (let columnIndex = 0; columnIndex < numColumns; ++columnIndex) {
      const value = (0, import_schema3.getTableCellAt)(table, rowIndex, columnIndex);
      const builder = builders[columnIndex];
      builder.append(value);
      batchLength++;
      if (batchLength >= batchSize) {
        const datas = builders.map((builder2) => builder2.flush());
        const structData = new arrow5.Data(structField, 0, batchLength, 0, void 0, datas);
        yield new arrow5.RecordBatch(arrowSchema, structData);
        batchLength = 0;
      }
    }
  }
  if (batchLength > 0) {
    const datas = builders.map((builder) => builder.flush());
    const structData = new arrow5.Data(structField, 0, batchLength, 0, void 0, datas);
    yield new arrow5.RecordBatch(arrowSchema, structData);
    batchLength = 0;
  }
  builders.map((builder) => builder.finish());
}

// dist/lib/geoarrow/convert-geoarrow-to-binary-geometry.js
var import_polygon = require("@math.gl/polygon");

// dist/lib/geoarrow/get-arrow-bounds.js
function updateBoundsFromGeoArrowSamples(flatCoords, nDim, bounds, sampleSize = 100) {
  const numberOfFeatures = flatCoords.length / nDim;
  const sampleStep = Math.max(Math.floor(numberOfFeatures / sampleSize), 1);
  const newBounds = [...bounds];
  for (let i = 0; i < numberOfFeatures; i += sampleStep) {
    const lng = flatCoords[i * nDim];
    const lat = flatCoords[i * nDim + 1];
    if (lng < newBounds[0]) {
      newBounds[0] = lng;
    }
    if (lat < newBounds[1]) {
      newBounds[1] = lat;
    }
    if (lng > newBounds[2]) {
      newBounds[2] = lng;
    }
    if (lat > newBounds[3]) {
      newBounds[3] = lat;
    }
  }
  return newBounds;
}

// dist/lib/geoarrow/convert-geoarrow-to-binary-geometry.js
var BinaryGeometryType;
(function(BinaryGeometryType2) {
  BinaryGeometryType2["points"] = "points";
  BinaryGeometryType2["lines"] = "lines";
  BinaryGeometryType2["polygons"] = "polygons";
})(BinaryGeometryType || (BinaryGeometryType = {}));
function getBinaryGeometryTemplate() {
  return {
    globalFeatureIds: { value: new Uint32Array(0), size: 1 },
    positions: { value: new Float32Array(0), size: 2 },
    properties: [],
    numericProps: {},
    featureIds: { value: new Uint32Array(0), size: 1 }
  };
}
function getBinaryGeometriesFromArrow(geoColumn, geoEncoding, options) {
  const featureTypes = {
    polygon: geoEncoding === "geoarrow.multipolygon" || geoEncoding === "geoarrow.polygon",
    point: geoEncoding === "geoarrow.multipoint" || geoEncoding === "geoarrow.point",
    line: geoEncoding === "geoarrow.multilinestring" || geoEncoding === "geoarrow.linestring"
  };
  const chunks = (options == null ? void 0 : options.chunkIndex) !== void 0 && (options == null ? void 0 : options.chunkIndex) >= 0 ? [geoColumn.data[options == null ? void 0 : options.chunkIndex]] : geoColumn.data;
  let bounds = [Infinity, Infinity, -Infinity, -Infinity];
  let globalFeatureIdOffset = (options == null ? void 0 : options.chunkOffset) || 0;
  const binaryGeometries = [];
  chunks.forEach((chunk) => {
    const { featureIds, flatCoordinateArray, nDim, geomOffset, triangles } = getBinaryGeometriesFromChunk(chunk, geoEncoding, options);
    const globalFeatureIds = new Uint32Array(featureIds.length);
    for (let i = 0; i < featureIds.length; i++) {
      globalFeatureIds[i] = featureIds[i] + globalFeatureIdOffset;
    }
    const binaryContent = {
      globalFeatureIds: { value: globalFeatureIds, size: 1 },
      positions: {
        value: flatCoordinateArray,
        size: nDim
      },
      featureIds: { value: featureIds, size: 1 },
      // eslint-disable-next-line no-loop-func
      properties: [...Array(chunk.length).keys()].map((i) => ({
        index: i + globalFeatureIdOffset
      }))
    };
    globalFeatureIdOffset += chunk.length;
    binaryGeometries.push({
      shape: "binary-feature-collection",
      points: {
        type: "Point",
        ...getBinaryGeometryTemplate(),
        ...featureTypes.point ? binaryContent : {}
      },
      lines: {
        type: "LineString",
        ...getBinaryGeometryTemplate(),
        ...featureTypes.line ? binaryContent : {},
        pathIndices: { value: featureTypes.line ? geomOffset : new Uint16Array(0), size: 1 }
      },
      polygons: {
        type: "Polygon",
        ...getBinaryGeometryTemplate(),
        ...featureTypes.polygon ? binaryContent : {},
        polygonIndices: {
          // use geomOffset as polygonIndices same as primitivePolygonIndices since we are using earcut to get triangule indices
          value: featureTypes.polygon ? geomOffset : new Uint16Array(0),
          size: 1
        },
        primitivePolygonIndices: {
          value: featureTypes.polygon ? geomOffset : new Uint16Array(0),
          size: 1
        },
        ...triangles ? { triangles: { value: triangles, size: 1 } } : {}
      }
    });
    bounds = updateBoundsFromGeoArrowSamples(flatCoordinateArray, nDim, bounds);
  });
  return {
    binaryGeometries,
    bounds,
    featureTypes,
    ...(options == null ? void 0 : options.calculateMeanCenters) ? { meanCenters: getMeanCentersFromBinaryGeometries(binaryGeometries) } : {}
  };
}
function getMeanCentersFromBinaryGeometries(binaryGeometries) {
  const globalMeanCenters = [];
  binaryGeometries.forEach((binaryGeometry) => {
    var _a;
    let binaryGeometryType = null;
    if (binaryGeometry.points && binaryGeometry.points.positions.value.length > 0) {
      binaryGeometryType = BinaryGeometryType.points;
    } else if (binaryGeometry.lines && binaryGeometry.lines.positions.value.length > 0) {
      binaryGeometryType = BinaryGeometryType.lines;
    } else if (binaryGeometry.polygons && binaryGeometry.polygons.positions.value.length > 0) {
      binaryGeometryType = BinaryGeometryType.polygons;
    }
    const binaryContent = binaryGeometryType ? binaryGeometry[binaryGeometryType] : null;
    if (binaryContent && binaryGeometryType !== null) {
      const featureIds = binaryContent.featureIds.value;
      const flatCoordinateArray = binaryContent.positions.value;
      const nDim = binaryContent.positions.size;
      const primitivePolygonIndices = binaryContent.type === "Polygon" ? (_a = binaryContent.primitivePolygonIndices) == null ? void 0 : _a.value : void 0;
      const meanCenters = getMeanCentersFromGeometry(featureIds, flatCoordinateArray, nDim, binaryGeometryType, primitivePolygonIndices);
      meanCenters.forEach((center) => {
        globalMeanCenters.push(center);
      });
    }
  });
  return globalMeanCenters;
}
function getMeanCentersFromGeometry(featureIds, flatCoordinateArray, nDim, geometryType, primitivePolygonIndices) {
  const meanCenters = [];
  const vertexCount = flatCoordinateArray.length;
  let vertexIndex = 0;
  let coordIdx = 0;
  let primitiveIdx = 0;
  while (vertexIndex < vertexCount) {
    const featureId = featureIds[vertexIndex / nDim];
    const center = [0, 0];
    let vertexCountInFeature = 0;
    while (vertexIndex < vertexCount && featureIds[coordIdx] === featureId) {
      if (geometryType === BinaryGeometryType.polygons && (primitivePolygonIndices == null ? void 0 : primitivePolygonIndices[primitiveIdx]) === coordIdx) {
        vertexIndex += nDim;
        primitiveIdx++;
      } else {
        center[0] += flatCoordinateArray[vertexIndex];
        center[1] += flatCoordinateArray[vertexIndex + 1];
        vertexIndex += nDim;
        vertexCountInFeature++;
      }
      coordIdx += 1;
    }
    center[0] /= vertexCountInFeature;
    center[1] /= vertexCountInFeature;
    meanCenters.push(center);
  }
  return meanCenters;
}
function getBinaryGeometriesFromChunk(chunk, geoEncoding, options) {
  switch (geoEncoding) {
    case "geoarrow.point":
    case "geoarrow.multipoint":
      return getBinaryPointsFromChunk(chunk, geoEncoding);
    case "geoarrow.linestring":
    case "geoarrow.multilinestring":
      return getBinaryLinesFromChunk(chunk, geoEncoding);
    case "geoarrow.polygon":
    case "geoarrow.multipolygon":
      return getBinaryPolygonsFromChunk(chunk, geoEncoding, options);
    default:
      throw Error("invalid geoarrow encoding");
  }
}
function getTriangleIndices(polygonIndices, primitivePolygonIndices, flatCoordinateArray, nDim) {
  try {
    let primitiveIndex = 0;
    const triangles = [];
    for (let i = 0; i < polygonIndices.length - 1; i++) {
      const startIdx = polygonIndices[i];
      const endIdx = polygonIndices[i + 1];
      const slicedFlatCoords = flatCoordinateArray.subarray(startIdx * nDim, endIdx * nDim);
      const holeIndices = [];
      while (primitivePolygonIndices[primitiveIndex] < endIdx) {
        if (primitivePolygonIndices[primitiveIndex] > startIdx) {
          holeIndices.push(primitivePolygonIndices[primitiveIndex] - startIdx);
        }
        primitiveIndex++;
      }
      const triangleIndices = (0, import_polygon.earcut)(slicedFlatCoords, holeIndices.length > 0 ? holeIndices : void 0, nDim);
      if (triangleIndices.length === 0) {
        throw Error("earcut failed e.g. invalid polygon");
      }
      for (let j = 0; j < triangleIndices.length; j++) {
        triangles.push(triangleIndices[j] + startIdx);
      }
    }
    const trianglesUint32 = new Uint32Array(triangles.length);
    for (let i = 0; i < triangles.length; i++) {
      trianglesUint32[i] = triangles[i];
    }
    return trianglesUint32;
  } catch (error) {
    return null;
  }
}
function getBinaryPolygonsFromChunk(chunk, geoEncoding, options) {
  const isMultiPolygon = geoEncoding === "geoarrow.multipolygon";
  const polygonData = isMultiPolygon ? chunk.children[0] : chunk;
  const polygonOffset = polygonData.valueOffsets;
  const partData = isMultiPolygon ? chunk.valueOffsets.map((i) => polygonOffset.at(i) || i) : chunk.valueOffsets;
  const ringData = polygonData.children[0];
  const pointData = ringData.children[0];
  const coordData = pointData.children[0];
  const nDim = pointData.stride;
  const geomOffset = ringData.valueOffsets;
  const flatCoordinateArray = coordData.values;
  const geometryIndicies = new Uint16Array(polygonOffset.length);
  for (let i = 0; i < polygonOffset.length; i++) {
    geometryIndicies[i] = geomOffset[polygonOffset[i]];
  }
  const numOfVertices = flatCoordinateArray.length / nDim;
  const featureIds = new Uint32Array(numOfVertices);
  for (let i = 0; i < partData.length - 1; i++) {
    const startIdx = geomOffset[partData[i]];
    const endIdx = geomOffset[partData[i + 1]];
    for (let j = startIdx; j < endIdx; j++) {
      featureIds[j] = i;
    }
  }
  const triangles = (options == null ? void 0 : options.triangulate) ? getTriangleIndices(geometryIndicies, geomOffset, flatCoordinateArray, nDim) : null;
  return {
    featureIds,
    nDim,
    flatCoordinateArray,
    geomOffset,
    geometryIndicies,
    ...(options == null ? void 0 : options.triangulate) && triangles ? { triangles } : {}
  };
}
function getBinaryLinesFromChunk(chunk, geoEncoding) {
  const isMultiLineString = geoEncoding === "geoarrow.multilinestring";
  const lineData = isMultiLineString ? chunk.children[0] : chunk;
  const pointData = lineData.children[0];
  const coordData = pointData.children[0];
  const nDim = pointData.stride;
  const geomOffset = lineData.valueOffsets;
  const flatCoordinateArray = coordData.values;
  const geometryIndicies = new Uint16Array(0);
  const numOfVertices = flatCoordinateArray.length / nDim;
  const featureIds = new Uint32Array(numOfVertices);
  if (isMultiLineString) {
    const partData = chunk.valueOffsets;
    for (let i = 0; i < partData.length - 1; i++) {
      const startIdx = geomOffset[partData[i]];
      const endIdx = geomOffset[partData[i + 1]];
      for (let j = startIdx; j < endIdx; j++) {
        featureIds[j] = i;
      }
    }
  } else {
    for (let i = 0; i < chunk.length; i++) {
      const startIdx = geomOffset[i];
      const endIdx = geomOffset[i + 1];
      for (let j = startIdx; j < endIdx; j++) {
        featureIds[j] = i;
      }
    }
  }
  return {
    featureIds,
    flatCoordinateArray,
    nDim,
    geomOffset,
    geometryIndicies
  };
}
function getBinaryPointsFromChunk(chunk, geoEncoding) {
  const isMultiPoint = geoEncoding === "geoarrow.multipoint";
  const pointData = isMultiPoint ? chunk.children[0] : chunk;
  const coordData = pointData.children[0];
  const nDim = pointData.stride;
  const flatCoordinateArray = coordData.values;
  const geometryIndicies = new Uint16Array(0);
  const geomOffset = new Int32Array(0);
  const numOfVertices = flatCoordinateArray.length / nDim;
  const featureIds = new Uint32Array(numOfVertices);
  if (isMultiPoint) {
    const partData = chunk.valueOffsets;
    for (let i = 0; i < partData.length - 1; i++) {
      const startIdx = partData[i];
      const endIdx = partData[i + 1];
      for (let j = startIdx; j < endIdx; j++) {
        featureIds[j] = i;
      }
    }
  } else {
    for (let i = 0; i < chunk.length; i++) {
      featureIds[i] = i;
    }
  }
  return {
    featureIds,
    flatCoordinateArray,
    nDim,
    geomOffset,
    geometryIndicies
  };
}

// dist/workers/hard-clone.js
var arrow6 = __toESM(require("apache-arrow"), 1);
function hardClone(data, force = false) {
  if ("data" in data) {
    return new arrow6.Vector(data.data.map((data2) => hardClone(data2, force)));
  }
  const clonedChildren = [];
  for (const childData of data.children) {
    clonedChildren.push(hardClone(childData, force));
  }
  let clonedDictionary;
  if (data.dictionary !== void 0) {
    clonedDictionary = hardClone(data.dictionary, force);
  }
  const clonedBuffers = {
    [arrow6.BufferType.OFFSET]: cloneBuffer(data.buffers[arrow6.BufferType.OFFSET], force),
    [arrow6.BufferType.DATA]: cloneBuffer(data.buffers[arrow6.BufferType.DATA], force),
    [arrow6.BufferType.VALIDITY]: cloneBuffer(data.buffers[arrow6.BufferType.VALIDITY], force),
    [arrow6.BufferType.TYPE]: cloneBuffer(data.buffers[arrow6.BufferType.TYPE], force)
  };
  return new arrow6.Data(
    data.type,
    data.offset,
    data.length,
    // @ts-expect-error _nullCount is protected. We're using it here to mimic
    // `Data.clone`
    data._nullCount,
    clonedBuffers,
    clonedChildren,
    clonedDictionary
  );
}
function isTypedArraySliced(arr) {
  return !(arr.byteOffset === 0 && arr.byteLength === arr.buffer.byteLength);
}
function cloneBuffer(arr, force) {
  if (arr === void 0) {
    return arr;
  }
  if (!force && !isTypedArraySliced(arr)) {
    return arr;
  }
  return arr.slice();
}

// dist/triangulate-on-worker.js
var import_worker_utils = require("@loaders.gl/worker-utils");
var VERSION3 = true ? "4.3.3" : "latest";
var TriangulationWorker = {
  id: "triangulation",
  name: "Triangulate",
  module: "arrow",
  version: VERSION3,
  options: {}
};
function triangulateOnWorker(data, options = {}) {
  return (0, import_worker_utils.processOnWorker)(TriangulationWorker, { ...data, operation: "triangulate" }, options);
}
function parseGeoArrowOnWorker(data, options = {}) {
  return (0, import_worker_utils.processOnWorker)(TriangulationWorker, { ...data, operation: "parse-geoarrow" }, options);
}

// dist/index.js
import_schema4.TableBatchBuilder.ArrowBatch = ArrowTableBatchAggregator;
//# sourceMappingURL=index.cjs.map
