// loaders.gl
// SPDX-License-Identifier: MIT
// Copyright (c) vis.gl contributors
import { ParquetReader } from "../../parquetjs/parser/parquet-reader.js";
import { materializeColumns } from "../../parquetjs/schema/shred.js";
import { getSchemaFromParquetReader } from "./get-parquet-schema.js";
import { installBufferPolyfill } from "../../polyfills/buffer/index.js";
import { preloadCompressions } from "../../parquetjs/compression.js";
/**
 * @deprecated
 */
export async function parseParquetFileInColumns(file, options) {
    installBufferPolyfill();
    await preloadCompressions(options);
    for await (const batch of parseParquetFileInColumnarBatches(file, options)) {
        return {
            shape: 'columnar-table',
            schema: batch.schema,
            data: batch.data
        };
    }
    throw new Error('empty table');
}
/**
 * @deprecated
 */
export async function* parseParquetFileInColumnarBatches(file, options) {
    installBufferPolyfill();
    await preloadCompressions(options);
    const reader = new ParquetReader(file);
    // Extract schema and geo metadata
    const schema = await getSchemaFromParquetReader(reader);
    const parquetSchema = await reader.getSchema();
    // Iterate over row batches
    const rowGroups = reader.rowGroupIterator(options?.parquet);
    for await (const rowGroup of rowGroups) {
        yield convertRowGroupToTableBatch(rowGroup, parquetSchema, schema);
    }
}
function convertRowGroupToTableBatch(rowGroup, parquetSchema, schema) {
    // const data = convertParquetRowGroupToColumns(schema, rowGroup);
    const data = materializeColumns(parquetSchema, rowGroup);
    return {
        shape: 'columnar-table',
        batchType: 'data',
        schema,
        data,
        length: rowGroup.rowCount
    };
}
